"""
Phase 2 Scraper (Scraper) for CPUs.

This script reads the 'cpu_links.txt' file generated by the Phase 1 crawler.
It then visits each individual CPU page, parses all specification tables,
and saves the cleaned data into a master JSON database file.

It includes features for:
- Automatic resuming from a partially completed database.
- Polite, randomized delays between requests.
- Automatic retries with a long cool-down period upon HTTP 429 (Too Many Requests).
- On-the-fly saving of progress.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
import os       
import random   

# --- Configuration ---
INPUT_FILE = "txt/cpu_links.txt"
OUTPUT_FILE = "json/master_cpu_database.json"
MIN_DELAY_SECONDS = 4
MAX_DELAY_SECONDS = 10
RETRY_DELAY_SECONDS = 300  # 5 minutes
# ---------------------

def get_detailed_specs(soup):
    """
    Parses all specification tables from a single CPU detail page.
    
    This parser is designed for the <table>, <th>, <td> structure
    used on TechPowerUp's CPU pages. It also handles the special
    <ul> structure for the "Features" section.

    :param soup: A BeautifulSoup object of the CPU detail page.
    :return: A dictionary of all found specs.
    """
    specs = {}
    
    spec_sections = soup.find_all('section', class_='details')
    
    for section in spec_sections:
        # The section title is in an <h1> tag
        section_title_tag = section.find('h1')
        if not section_title_tag:
            continue
            
        section_title = section_title_tag.text.strip()
        
        # --- Handle Special "Features" Section (which uses <ul>) ---
        if section_title == "Features":
            features_list = section.find_all('li')
            if features_list:
                specs['Features'] = ", ".join([f.text.strip() for f in features_list])
            continue # Move to the next section
            
        # --- Handle All Other Table-Based Sections ---
        table = section.find('table')
        if not table:
            continue
            
        rows = table.find_all('tr')
        
        for row in rows:
            header_cell = row.find('th')
            data_cell = row.find('td')
            
            if header_cell and data_cell:
                # Clean the key: "Socket:" -> "Socket"
                spec_name = header_cell.get_text(strip=True).replace(":", "")
                
                # Get clean value, replacing <br> with a space
                spec_value = data_cell.get_text(separator=' ', strip=True)
                
                # --- Clean up known messy values ---
                if spec_name == "Launch Price":
                    spec_value = spec_value.replace("$", "").replace(",", "").replace("USD", "").strip()
                
                # Remove common units
                spec_value = spec_value.replace(" W", "")
                spec_value = spec_value.replace(" GB/s", "")
                spec_value = spec_value.replace(" GB", "")
                spec_value = spec_value.replace(" MB", "")
                spec_value = spec_value.replace(" KB", "")
                spec_value = spec_value.replace(" GHz", "")
                spec_value = spec_value.replace(" MHz", "")
                spec_value = spec_value.replace(" MT/s", "")
                spec_value = spec_value.replace(" nm", "")
                spec_value = spec_value.replace(" mm²", "")
                spec_value = spec_value.replace("°C", "")
                
                # Clean up prefixes and parentheticals
                spec_value = re.sub(r"^up to ", "", spec_value) # "up to 4.6" -> "4.6"
                spec_value = re.sub(r"\s*\([^)]+\)$", "", spec_value).strip() # "32 (shared)" -> "32"

                # Use a combined key, e.g., "Performance - Frequency"
                clean_key = f"{section_title} - {spec_name}"
                specs[clean_key] = spec_value
    return specs

def run_scraper():
    """
    Main function to run the CPU scraper.
    
    Loads the URL list, loads any existing database to resume,
    then iterates through all URLs, scraping and saving data.
    """
    master_database = []
    scraped_urls = set()

    # --- 1. Automatic Resume Logic ---
    if os.path.exists(OUTPUT_FILE):
        print(f"Found existing database '{OUTPUT_FILE}'. Loading...")
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                master_database = json.load(f)
                for item in master_database:
                    if 'URL' in item:
                        scraped_urls.add(item['URL'])
            print(f"Loaded {len(master_database)} items. Will skip these URLs.")
        except json.JSONDecodeError:
            print(f"Warning: '{OUTPUT_FILE}' is corrupt. Starting fresh.")
            master_database = []
        except Exception as e:
            print(f"Error loading {OUTPUT_FILE}: {e}. Starting fresh.")
            master_database = []

    # --- 2. Load "Hit-List" ---
    try:
        with open(INPUT_FILE, 'r') as f:
            urls_to_scrape = [line.strip() for line in f.readlines() if line.strip() and "https://" in line]
        print(f"Loaded {len(urls_to_scrape)} total links from {INPUT_FILE}.")
    except FileNotFoundError:
        print(f"❌ ERROR: '{INPUT_FILE}' not found. Make sure it's in the same directory.")
        return
    except Exception as e:
        print(f"❌ ERROR: Could not read '{INPUT_FILE}': {e}")
        return

    # --- 3. Main Scraping Loop ---
    total_links = len(urls_to_scrape)
    new_items_scraped = 0
    
    for i, url in enumerate(urls_to_scrape):
        
        # Check if this URL was already scraped (from resume logic)
        if url in scraped_urls:
            print(f"Skipping {i+1}/{total_links} (already in database): {url}")
            continue

        print(f"Scraping {i+1}/{total_links}: {url}")
        
        # --- 4. Automatic Retry Loop (per-URL) ---
        while True: # Keep trying this URL until it succeeds or fails permanently
            try:
                # Add a randomized "human" delay
                delay = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)
                print(f"   ...waiting {delay:.1f} seconds...")
                time.sleep(delay)
                
                response = requests.get(url)
                response.raise_for_status() # Triggers 'except' for 4xx/5xx errors
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Find the CPU name
                cpu_name_tag = soup.find('h1', class_='cpuname')
                if not cpu_name_tag:
                    print(f"  ⚠️ Could not find 'cpuname' tag, skipping.")
                    break # Break retry loop, move to next URL
                    
                cpu_name = cpu_name_tag.text.strip()
                
                # Parse all specs from the page
                detailed_specs = get_detailed_specs(soup)
                detailed_specs['Name'] = cpu_name
                detailed_specs['URL'] = url
                
                master_database.append(detailed_specs)
                new_items_scraped += 1
                
                # --- 6. Save Progress Immediately ---
                with open(OUTPUT_FILE, "w", encoding='utf-8') as f:
                    json.dump(master_database, f, indent=4, ensure_ascii=False)
                
                print(f"  > Success! Saved {cpu_name}.")
                break # Success! Break retry loop, move to next URL

            except requests.exceptions.HTTPError as e:
                # Handle "Too Many Requests" error specifically
                if e.response.status_code == 429:
                    print(f"  ❌ GOT A 429 ERROR! (Too Many Requests)")
                    print(f"     Server told us to cool off. Sleeping for {RETRY_DELAY_SECONDS} seconds...")
                    time.sleep(RETRY_DELAY_SECONDS)
                    print(f"     ...Waking up. Retrying *same* URL...")
                    continue # Restarts the 'while True' loop for this *same URL*
                else:
                    print(f"  ❌ FAILED with HTTP Error {e.response.status_code}: {e}")
                    break # Break retry loop, skip to next URL

            except Exception as e:
                print(f"  ❌ FAILED to scrape {url}: {e}")
                break # Break retry loop, skip to next URL

    # --- 7. Final Summary ---
    print(f"\n--- SCRAPE COMPLETE ---")
    print(f"Found {new_items_scraped} new CPUs.")
    print(f"Total CPUs in database: {len(master_database)}")
    print(f"Master Database saved to '{OUTPUT_FILE}'.")

# --- Run the main script ---
if __name__ == "__main__":
    run_scraper()