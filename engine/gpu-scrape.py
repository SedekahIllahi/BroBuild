"""
Phase 2 Scraper (Scraper) for GPUs.

This script reads the 'gpu_links.txt' file generated by the Phase 1 crawler.
It then visits each individual GPU page, parses all specification tables,
and saves the cleaned data into a master JSON database file.

It includes features for:
- Automatic resuming from a partially completed database.
- Polite, randomized delays between requests.
- Automatic retries with a long cool-down period upon HTTP 429 (Too Many Requests).
- On-the-fly saving of progress.
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
import os       
import random   

# --- Configuration ---
INPUT_FILE = "txt/gpu_links.txt"
OUTPUT_FILE = "json/master_gpu_database.json"
MIN_DELAY_SECONDS = 4
MAX_DELAY_SECONDS = 10
RETRY_DELAY_SECONDS = 300  # 5 minutes
# ---------------------

def get_detailed_specs(soup):
    """
    Parses all specification tables from a single GPU detail page.
    
    This parser is designed for the <section>, <dl>, <dt>, <dd> structure
    used on TechPowerUp's GPU pages.

    :param soup: A BeautifulSoup object of the GPU detail page.
    :return: A dictionary of all found specs.
    """
    specs = {}
    
    spec_sections = soup.find_all('section', class_='details')
    
    for section in spec_sections:
        section_title_tag = section.find('h2')
        if section_title_tag:
            section_title = section_title_tag.text.strip()
            
            rows = section.find_all('dl')
            
            for row in rows:
                header_cell = row.find('dt')
                data_cell = row.find('dd')
                
                if header_cell and data_cell:
                    spec_name = header_cell.text.strip()
                    spec_value = data_cell.text.strip()
                    
                    # --- Clean up known messy values ---
                    if spec_name == "Launch Price":
                        spec_value = spec_value.replace(" USD", "").replace(",", "")
                    if "TDP" in spec_name:
                        spec_value = re.sub(r"(\d+).*", r"\1", spec_value) # "160 W" -> "160"
                    if "GB" in spec_value:
                        spec_value = spec_value.replace(" GB", "")
                    if "MHz" in spec_value:
                        spec_value = re.sub(r"(\d+).*", r"\1", spec_value) # "1920 MHz" -> "1920"
                    
                    # Use a combined key, e.g., "Graphics Processor - GPU Name"
                    clean_key = f"{section_title} - {spec_name}"
                    specs[clean_key] = spec_value
    return specs

def run_scraper():
    """
    Main function to run the GPU scraper.
    
    Loads the URL list, loads any existing database to resume,
    then iterates through all URLs, scraping and saving data.
    """
    master_database = []
    scraped_urls = set()

    # --- 1. Automatic Resume Logic ---
    if os.path.exists(OUTPUT_FILE):
        print(f"Found existing database '{OUTPUT_FILE}'. Loading...")
        try:
            with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
                master_database = json.load(f)
                for item in master_database:
                    if 'URL' in item:
                        scraped_urls.add(item['URL'])
            print(f"Loaded {len(master_database)} items. Will skip these URLs.")
        except json.JSONDecodeError:
            print(f"Warning: '{OUTPUT_FILE}' is corrupt. Starting fresh.")
            master_database = []
        except Exception as e:
            print(f"Error loading {OUTPUT_FILE}: {e}. Starting fresh.")
            master_database = []

    # --- 2. Load "Hit-List" ---
    try:
        with open(INPUT_FILE, 'r') as f:
            urls_to_scrape = [line.strip() for line in f.readlines() if line.strip()]
        print(f"Loaded {len(urls_to_scrape)} total links from {INPUT_FILE}.")
    except FileNotFoundError:
        print(f"❌ ERROR: '{INPUT_FILE}' not found. Did you run the Phase 1 script first?")
        return

    # --- 3. Main Scraping Loop ---
    total_links = len(urls_to_scrape)
    new_items_scraped = 0
    
    for i, url in enumerate(urls_to_scrape):
        
        # Check if this URL was already scraped (from resume logic)
        if url in scraped_urls:
            print(f"Skipping {i+1}/{total_links} (already in database): {url}")
            continue

        print(f"Scraping {i+1}/{total_links}: {url}")
        
        # --- 4. Automatic Retry Loop (per-URL) ---
        while True: # Keep trying this URL until it succeeds or fails permanently
            try:
                # Add a randomized "human" delay
                delay = random.uniform(MIN_DELAY_SECONDS, MAX_DELAY_SECONDS)
                print(f"   ...waiting {delay:.1f} seconds...")
                time.sleep(delay)
                
                response = requests.get(url)
                response.raise_for_status() # Triggers 'except' for 4xx/5xx errors
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Find the GPU name
                gpu_name_tag = soup.find('h1', class_='gpudb-name')
                if not gpu_name_tag:
                    print(f"  ⚠️ Could not find 'gpudb-name' tag, skipping.")
                    break # Break retry loop, move to next URL
                    
                gpu_name = gpu_name_tag.text.strip()
                
                # Parse all specs from the page
                detailed_specs = get_detailed_specs(soup)
                detailed_specs['Name'] = gpu_name
                detailed_specs['URL'] = url
                
                master_database.append(detailed_specs)
                new_items_scraped += 1
                
                # --- 6. Save Progress Immediately ---
                with open(OUTPUT_FILE, "w", encoding='utf-8') as f:
                    json.dump(master_database, f, indent=4, ensure_ascii=False)
                
                print(f"  > Success! Saved {gpu_name}.")
                break # Success! Break retry loop, move to next URL

            except requests.exceptions.HTTPError as e:
                # Handle "Too Many Requests" error specifically
                if e.response.status_code == 429:
                    print(f"  ❌ GOT A 429 ERROR! (Too Many Requests)")
                    print(f"     Server told us to cool off. Sleeping for {RETRY_DELAY_SECONDS} seconds...")
                    time.sleep(RETRY_DELAY_SECONDS)
                    print(f"     ...Waking up. Retrying *same* URL...")
                    continue # Restarts the 'while True' loop for this *same URL*
                else:
                    print(f"  ❌ FAILED with HTTP Error {e.response.status_code}: {e}")
                    break # Break retry loop, skip to next URL

            except Exception as e:
                print(f"  ❌ FAILED to scrape {url}: {e}")
                break # Break retry loop, skip to next URL

    # --- 7. Final Summary ---
    print(f"\n--- SCRAPE COMPLETE ---")
    print(f"Found {new_items_scraped} new GPUs.")
    print(f"Total GPUs in database: {len(master_database)}")
    print(f"Master Database saved to '{OUTPUT_FILE}'.")

# --- Run the main script ---
if __name__ == "__main__":
    run_scraper()